{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf0cd082",
   "metadata": {},
   "source": [
    "# Fine Tuning usando Unsloth library\n",
    "C√≥digo baseado em https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing#scrollTo=2eSvM9zX_2d3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3426c172-c7b1-4106-b421-ffb8784d180e",
   "metadata": {},
   "source": [
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763148bb-a64b-4cb1-a858-b2953b2abe8f",
   "metadata": {},
   "source": [
    "!pip uninstall unsloth -y\n",
    "!pip uninstall peft -y\n",
    "!pip uninstall accelerate -y\n",
    "!pip uninstall bitsandbytes -y\n",
    "!pip uninstall trl -y\n",
    "!pip uninstall xformers -y\n",
    "\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b5ded8-c6f6-4208-adfb-2d17573d4665",
   "metadata": {},
   "source": [
    "Configurando ambiente\n",
    "\n",
    "sudo apt install python3-pip\n",
    "pip install --upgrade pip\n",
    "python.exe -m pip install --upgrade pip\n",
    "pip install notebook\n",
    "restart\n",
    "jupyter notebook\n",
    "\n",
    "pip3 install torch torchvision torchaudio\n",
    "pip install tensorflow\n",
    "pip install transformers\n",
    "pip install git+https://github.com/Keith-Hon/bitsandbytes-windows.git\n",
    "pip install accelerate\n",
    "pip install appdirs==1.4.4\n",
    "pip install datasets\n",
    "pip install fire==0.5.0\n",
    "pip install sentencepiece==0.1.97\n",
    "pip install peft\n",
    "pip install tensorboardX==2.6\n",
    "pip install gradio==3.23.0\n",
    "pip install seaborn\n",
    "pip install wandb\n",
    "pip install langchain\n",
    "pip install langchain-community\n",
    "pip install langchain_huggingface\n",
    "pip install chromadb\n",
    "pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "537e8273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 10:57:35.127128: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-01 10:57:35.134621: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-01 10:57:35.142505: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-01 10:57:35.144815: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-01 10:57:35.152031: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-01 10:57:35.629528: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name = unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import re\n",
    "from llm_utils import salvar_modelo, carregar_modelo, pick_from_dataset, carregar_dataset, alpaca_prompt, print_memory_used_in_training, print_memory_usage\n",
    "    \n",
    "RESUME_FROM_CHECKPOINT = False\n",
    "CONTINUAR_TREINAMENTO_SALVO = True\n",
    "pasta_base = './Datasets/novo-rel-voto-ementa/'\n",
    "DATASET_FILE = pasta_base + 'dataset_rel_voto0.json'\n",
    "BATCH_START = 0\n",
    "BATCH_END = 0 # 0 means there is no end\n",
    "BATCH_SIZE = BATCH_END - BATCH_START \n",
    "\n",
    "MY_MODEL_NAME = \"./fine_tuned_models/relvoto_unsloth_lora_model_llama_31_8b_4bit_pt\"\n",
    "max_seq_length = 10000 #8k roda - 12k estoura\n",
    "\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "def formatar_paragrafos_sucessivos(texto):\n",
    "    pattern = '(?<=[0-9\"‚Äù\\)√Ä-√ú√†-√ºAEILMORSUXYZaeilmorsuxyz]\\.) {0,4}\"?(?=[0-9]{1,2} ?[\\‚Äì\\-\\.] ?[A-Z√Ä-√ú])'\n",
    "    return re.sub(pattern, '\\n\\n', texto)\n",
    "    \n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "# Lista de modelos do site https://github.com/unslothai/unsloth?tab=readme-ov-file em 2024_07_28\n",
    "fourbit_models_2024_07_28 = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",          # Phi-3 2x faster!d\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "MODEL_NAME = \"unsloth/llama-3-8b-bnb-4bit\"\n",
    "MODEL_NAME = fourbit_models[4] # \"unsloth/llama-3-70b-bnb-4bit\" - n√£o funcionou. faltou mem√≥ria\n",
    "MODEL_NAME = fourbit_models_2024_07_28[1]\n",
    "\n",
    "print('Model name =', MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f541e-b2ca-443d-bd9b-608bfc2429b8",
   "metadata": {},
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf6e15e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vamos continuar o treinamento do: ./fine_tuned_models/relvoto_unsloth_lora_model_llama_31_8b_4bit_pt\n",
      "==((====))==  Unsloth: Fast Llama patching release 2024.8\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo ./fine_tuned_models/relvoto_unsloth_lora_model_llama_31_8b_4bit_pt carregado.\n",
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.988 GB.\n",
      "6.141 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "if CONTINUAR_TREINAMENTO_SALVO:\n",
    "    model_name = MY_MODEL_NAME\n",
    "    criar_camada_LORA = False\n",
    "    print('Vamos continuar o treinamento do:', MY_MODEL_NAME)\n",
    "else:\n",
    "    model_name = MODEL_NAME\n",
    "    criar_camada_LORA = True\n",
    "    \n",
    "model, tokenizer = carregar_modelo(model_name = model_name, \n",
    "                               max_seq_length = max_seq_length, \n",
    "                               dtype = dtype, \n",
    "                               load_in_4bit = load_in_4bit,\n",
    "                               criar_camada_LORA = criar_camada_LORA)\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125ed730-7329-4595-883b-ae36c159218a",
   "metadata": {},
   "source": [
    "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.988 GB.\n",
    "5.77 GB of memory reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f031088f-8ece-464b-8f63-543485ebfb93",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da4665ee-0f17-45bc-a869-162a21de9d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'desembargador', 'text'],\n",
      "    num_rows: 9031\n",
      "})\n",
      "eval_dataset: Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'desembargador', 'text'],\n",
      "    num_rows: 128\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset, eval_dataset = carregar_dataset(data_files = DATASET_FILE, eos_token = tokenizer.eos_token)\n",
    "# offset\n",
    "if BATCH_END != 0:\n",
    "    train_dataset = pick_from_dataset(train_dataset, \n",
    "                                      batch_start = BATCH_START, \n",
    "                                      batch_end = BATCH_END)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21f8611-36d1-40ab-bc07-8bffa55814ba",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6e56e5-16b6-4fa7-a7f4-08652b9be62f",
   "metadata": {},
   "source": [
    "Now let's use Huggingface TRL's SFTTrainer! More docs here: TRL SFT docs. We do 60 steps to speed things up, but you can set num_train_epochs=1 for a full run, and turn off max_steps=None. We also support TRL's DPOTrainer!\n",
    "\n",
    "Para determinar o tamanho do treino, posso usar um destes:\n",
    "- max_steps = BATCH_SIZE\n",
    "- num_train_epochs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ab25ca8-2ce5-46a1-8f94-9590a4d10676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d303132a304675b507ae83408c599d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/9031 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6a2bfb61244f528784302496b99a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 10,\n",
    "        do_eval = False, #Estou com erro de mem√≥ria ao converter para FP32 ou FP16 durante evaluate.\n",
    "        save_steps = 200,\n",
    "        load_best_model_at_end = False, # N√£o posso usar evaluate com quantiza√ß√£o (memory)\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to=\"wandb\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49229734-9de1-405a-a7d6-8a30468bfc9e",
   "metadata": {},
   "source": [
    "# Show current memory stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0714f2f6-ed26-48b5-8ade-7107d161a3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.988 GB.\n",
      "6.141 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d60d997-039f-44b2-8769-d04c7c9ded44",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e42750a0-ae70-4afd-9cf7-7c77a8b199c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9031\n",
      "128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 9,031 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 16 | Total steps = 564\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfabiocollado\u001b[0m (\u001b[33mfabiocollado-unicamp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ef72d4288148ed85ba3caa5d0ba9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112296911111723, max=1.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fabio/2_TRF/Projetos/Llama/llama-hf/wandb/run-20241001_105813-f2m0mka8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fabiocollado-unicamp/huggingface/runs/f2m0mka8' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/fabiocollado-unicamp/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fabiocollado-unicamp/huggingface' target=\"_blank\">https://wandb.ai/fabiocollado-unicamp/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fabiocollado-unicamp/huggingface/runs/f2m0mka8' target=\"_blank\">https://wandb.ai/fabiocollado-unicamp/huggingface/runs/f2m0mka8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='224' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [224/564 1:55:55 < 2:57:32, 0.03 it/s, Epoch 0.40/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.409500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.432200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.425600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.371200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.412200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.379500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.424900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.458400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.464400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.436300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.383800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.429500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.433400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.399100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.450100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.385900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.444500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.400300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.412100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.411000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:3\u001b[0m\n",
      "File \u001b[0;32m<string>:126\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m<string>:363\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/accelerator.py:2159\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2159\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(len(train_dataset))\n",
    "print(len(eval_dataset))\n",
    "trainer_stats = trainer.train(resume_from_checkpoint = RESUME_FROM_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cc49b1-2833-455b-9f01-5606af19346a",
   "metadata": {},
   "source": [
    "# Show final memory and time stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e108365-1ef4-4004-9f01-1606dab36cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_memory_used_in_training(trainer_stats = trainer_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fac7808-981a-4ea6-b8c2-c07e26b6c0e9",
   "metadata": {},
   "source": [
    "# Saving finetuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794f6614-9a94-4eb1-ba13-f85de61a523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "salvar_modelo(my_model_name = MY_MODEL_NAME, \n",
    "              model = model, \n",
    "              tokenizer = tokenizer)\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e36fd3-c259-45e6-82b0-ad0053fcc148",
   "metadata": {},
   "source": [
    "# Loading finetuned models and Eval Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ecb194-4717-43ba-8bc0-fb82856217ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = carregar_modelo(model_name = MY_MODEL_NAME, \n",
    "                                   max_seq_length = max_seq_length, \n",
    "                                   dtype = dtype, \n",
    "                                   load_in_4bit = load_in_4bit,\n",
    "                                   criar_camada_LORA = False,\n",
    "                                   inference = True)\n",
    "print_memory_usage()\n",
    "\n",
    "_, eval_dataset = carregar_dataset(data_files = DATASET_FILE, eos_token = tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7248f40-8e6a-4067-8f3d-131f15e81805",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8589f13-a2a0-46b6-a950-cf3186d3eb34",
   "metadata": {},
   "source": [
    "# Inference\r\n",
    "Let's run the model! You can change the instruction and input - leave the output blank!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d22ac4-fa17-44dc-887b-04495d965867",
   "metadata": {},
   "source": [
    "### For inference, use:\n",
    "- inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Continue the fibonnaci sequence.\", # instruction\n",
    "        \"1, 1, 2, 3, 5, 8\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "- outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "- tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b29f833-ef2c-4adb-bef0-4df1cd3886bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "# Lembre-se que o dataset esta no formato {'instruction': ['Crie uma ementa para o seguinte voto:',  \n",
    "# 'Crie uma ementa para o seguinte voto:'], ...,  'input': [...], 'output': [...], 'text':[...]}\n",
    "\n",
    "def remove_list_of_words(text, list_of_words):\n",
    "    for word in list_of_words:\n",
    "        text = text.replace(word,'')\n",
    "    return text\n",
    "def inference_test(model, tokenizer, dataset, start, end, alpaca_prompt, tamanho_maximo_da_ementa):\n",
    "    sub_list = dataset[start:end]\n",
    "    for instruction, input, output in zip(sub_list['instruction'], sub_list['input'], sub_list['output']):\n",
    "        print('-----------------------------------------INPUT-----------------------------------------------------')\n",
    "        print(input)\n",
    "        print('-------------------------------------DESIRED-OUTPUT------------------------------------------------')\n",
    "        print(formatar_paragrafos_sucessivos(output))\n",
    "        print('------------------------------------GENERATED-OUTPUT-----------------------------------------------')\n",
    "        prompt = tokenizer([alpaca_prompt.format(instruction, input, '')], return_tensors = \"pt\").to(\"cuda\")\n",
    "        generated_output = model.generate(**prompt, max_new_tokens = tamanho_maximo_da_ementa, use_cache = True)\n",
    "        generated_output = tokenizer.batch_decode(generated_output)\n",
    "        texto = remove_list_of_words(generated_output[0], ['<|end_of_text|>', '</s>', '<|eot_id|>'])\n",
    "        if 'Response:\\n' in generated_output[0]:\n",
    "            response_word = 'Response:\\n'\n",
    "        elif 'Resposta:\\n' in generated_output[0]:\n",
    "            response_word = 'Resposta:\\n'\n",
    "        else:\n",
    "            raise Exception(\"Resposta n√£o encontrada no texto gerado. Ou formato inv√°lido do texto gerado.\")\n",
    "        print(formatar_paragrafos_sucessivos(texto.split(response_word)[1]))\n",
    "        print('---------------------------------------------------------------------------------------------------')\n",
    "        print()\n",
    "\n",
    "inference_test(model = model,\n",
    "               dataset = eval_dataset, start = 0, end = 127,\n",
    "               tokenizer = tokenizer,\n",
    "               alpaca_prompt = alpaca_prompt,\n",
    "               tamanho_maximo_da_ementa = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b17b36-92c0-43a1-bc26-9c2c81bf6df8",
   "metadata": {},
   "source": [
    "You can also use a TextStreamer for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ab017-6652-4c05-a4ef-84c4e4b12d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Continue the fibonnaci sequence.\", # instruction\n",
    "        \"1, 1, 2, 3, 5, 8\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c6f23-1d70-4955-b641-31c482efeb26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
